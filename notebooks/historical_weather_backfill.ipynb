{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical Weather Data Using PAIRS API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Use this file to fill empty visibility and Temperature values ONLY if you're using the Kaggle dataset here - https://www.kaggle.com/sobhanmoosavi/us-accidents\n",
    "\n",
    "DO NOT Run this notebook if you're using the dataset we provided here - ***. \n",
    "This has the temperature and visibility values filled already.\n",
    "\n",
    "### Environment Setup\n",
    "\n",
    "Run `pip install ibmpairs -U ` and ensure the version is 0.1.3 as this has the latest Auth features.\n",
    "\n",
    "API quickstart guide: https://pairs.res.ibm.com/tutorial/tutorials/api/quickstart.html\n",
    "\n",
    "\n",
    "Dataset Explorer: https://ibmpairs.mybluemix.net/data-explorer\n",
    "\n",
    "Login Credentials: \n",
    "    * username - ****\n",
    "    * password - ****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibmpairs import paw, authentication\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "from tzwhere import tzwhere\n",
    "from datetime import datetime   \n",
    "import pytz\n",
    "from timezonefinder import TimezoneFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAIRS_SERVER = '****'\n",
    "PAIRS_API_KEY = '***'\n",
    "PAIRS_CREDENTIALS = authentication.OAuth2(api_key = PAIRS_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query parameters of interest:\n",
    "\n",
    "- layer_id: the number associated with the dataset layer you are interested in. In the data explorer, find a dataset you want to query and click \"Data Layers\" to get the list of datalayers for that dataset. This is the ID you will use in the query\n",
    "- coordinates: (latitude, longitude) of the point you want data for\n",
    "- snapshot: the timestamp of the point you want data for. If data is missing for the specific timestamp, snapshot will grab the closest timestamp that has data\n",
    "\n",
    "Data from the query is returned in the form of a dataframe with the metadata. The `value` column is the one of interest here, as this shows the data point. The units of the observations can be found in the data explorer window for the specific layer that you queried."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Values we need to get from API\n",
    "\n",
    "- Temperature (F)\n",
    "    - layer 48552 temperature of air 2m above surface (6 hourly)\n",
    "    - layer 49257 temperature above ground (K) hourly\n",
    "- wind chill (F)\n",
    "    - layer 49310 temperature feels like (wind chill in winter and heat index in summer from humidity) in K\n",
    "- humidity (%)\n",
    "    - layer 49252 relative humidity of the air (%) hourly\n",
    "- pressure (in)\n",
    "    - layer 48544 surface pressure (units are in Pa)\n",
    "- Visibility (mi)\n",
    "    - layer 49312 visibility surface (m) but visibility >10 statute miles is 999 0 - hourly\n",
    "- wind direction\n",
    "    - layer 50463 wind direction (10 degree intervals where 0 is N, 90 is E, 180 is S, 270 is W) hourly\n",
    "- wind speed (mph)\n",
    "    - layer 48657 or 48868 max wind gust speed 10m above surface\n",
    "    - layer 49313 wind speed (m/s) hourly\n",
    "- precipitation (in) \n",
    "    - layer 26012  hourly recipitation rate\n",
    "    - layer 48547 snow depth (ECMWF)\n",
    "    - layer 48866 snowfall (6 hour interval)\n",
    "    - layer 49249 precipitation past 1 hr (mm)\n",
    "    - layer 49254 snow past 1 hour (hourly) (m)\n",
    "- weather condition (text) i.e. fair, cloudy, rainy\n",
    "\n",
    "- **layer 49314** driving difficulty on a scale of 0 to 100 that takes into account wind, precipitation and fog, hoursly data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('US_Accidents_Dec20_updated.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in missing timezones based on lat/long coordinates\n",
    "tf = TimezoneFinder()\n",
    "df['Timezone'] = df.apply(lambda row: tf.timezone_at(lng=row['Start_Lng'],lat=row['Start_Lat']) if pd.isna(row['Timezone']) else row['Timezone'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize timezone names used in pytz package\n",
    "timezone_map = {'US/Eastern':'America/New_York', 'US/Central':'America/Chicago', 'US/Mountain':'America/Denver', 'US/Pacific': 'America/Los_Angeles'}\n",
    "df['Timezone'].replace(timezone_map, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a timestamp to UTC using the timezone name\n",
    "def local_to_utc(local, timestamp):\n",
    "    local = pytz.timezone(local)\n",
    "    naive = datetime.strptime(timestamp, \"%Y-%m-%d %H:%M:%S\")\n",
    "    local_dt = local.localize(naive, is_dst=False)\n",
    "    utc_dt = local_dt.astimezone(pytz.utc)\n",
    "    utc_str = utc_dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    return utc_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop points with timezone not in the US\n",
    "df = df[df['Timezone'] != 'other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing timestamps\n",
    "df = df[df['Timezone'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove tailing decimal from timestamp\n",
    "def clean_timestamp(timestamp):\n",
    "    if '.' in timestamp:\n",
    "        return timestamp.split('.')[0]\n",
    "    else:\n",
    "        return timestamp\n",
    "df['Start_Time'] = df['Start_Time'].apply(clean_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create UTC timestamp column using the local timestamp and timezone\n",
    "df['UTC_timestamp'] =  df.apply(lambda row: local_to_utc(row['Timezone'], row['Start_Time']), axis =1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Missing Temperature Data Using PAIRS API Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a kelvin temperature to fahrenheit\n",
    "def kelvin_to_fahrenheit(kelvin):\n",
    "    return ((kelvin - 273.15) * (9/5) + 32)\n",
    "\n",
    "def m_to_mi(meters):\n",
    "    # 999 represents > 10 miles so return 10 miles\n",
    "    if meters == 999:\n",
    "        return 10\n",
    "    return (meters/1609.34)\n",
    "\n",
    "# function to perform a PAIRS query and get the temperature for a given point/timestamp\n",
    "def get_missing_vals(utc_timestamp, lat, long):\n",
    "    query_json = {\n",
    "                'layers': [\n",
    "                    {'id': 49257},\n",
    "                    {'id': 49312},\n",
    "                    {'id': 49314}\n",
    "                ],\n",
    "                \"spatial\" : {'type': 'point', 'coordinates': [lat, long]},\n",
    "                'temporal' : {'intervals' : [\n",
    "                    {'snapshot': utc_timestamp}\n",
    "                ]}\n",
    "            }\n",
    "\n",
    "    query = paw.PAIRSQuery(query_json, PAIRS_SERVER, PAIRS_CREDENTIALS, authType = 'api-key')\n",
    "    query.submit()\n",
    "\n",
    "    tdf = query.vdf\n",
    "    temp_k = tdf.iloc[0].value\n",
    "    visibility_m = tdf.iloc[1].value\n",
    "    driving_score = tdf.iloc[2].value\n",
    "    print(temp_k,visibility_m)\n",
    "\n",
    "    return (kelvin_to_fahrenheit(temp_k), m_to_mi(visibility_m), driving_score)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_dataframe(missing_df, idx):\n",
    "    for i in missing_df.index:\n",
    "        try:\n",
    "            time = missing_df.at[i, 'UTC_timestamp']\n",
    "            lat = missing_df.at[i, 'Start_Lat']\n",
    "            long = missing_df.at[i, 'Start_Lng']\n",
    "            temp, visibility, driving_score = get_missing_vals(time, lat, long)\n",
    "            missing_df.at[i, 'Temperature(F)'] = temp\n",
    "            missing_df.at[i, 'Visibility(mi)'] = visibility\n",
    "            missing_df.at[i, 'driving_score'] = driving_score\n",
    "        except Exception as e:\n",
    "            print(\"Caught exception : \", e)\n",
    "            raise AuthException(missing_df, idx)\n",
    "    return missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Exception class\n",
    "class AuthException(Exception):\n",
    "    def __init__(self, df, idx):\n",
    "        self.df = df\n",
    "        self.idx = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv('US_Accidents_Dec20_updated.csv')\n",
    "merged_df = original_df.merge(result_df[['ID', 'Temperature(F)', 'Visibility(mi)']], how = 'left', on = 'ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidate merged temperature and visibility data to 1 column\n",
    "merged_df['Temperature(F)'] = merged_df.apply(lambda row: row['Temperature(F)_y'] if pd.isna(row['Temperature(F)_x']) else row['Temperature(F)_x'], axis =1)\n",
    "merged_df['Visibility(mi)'] = merged_df.apply(lambda row: row['Visibility(mi)_y'] if pd.isna(row['Visibility(mi)_x']) else row['Visibility(mi)_x'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(['Temperature(F)_x', 'Visibility(mi)_x', 'Visibility(mi)_y', 'Temperature(F)_y','Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1'  ], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename this file to US_Accidents_Dec20_updated.csv to be used in later stages.\n",
    "merged_df.to_csv('updated.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
